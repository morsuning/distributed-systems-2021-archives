6.824 2021 第1讲：介绍

6.824：分布式系统工程

什么是分布式系统？
  多个协作计算机
  大型网站存储、MapReduce、点对点共享等
  大量关键基础设施是分布式的

为什么人们构建分布式系统？
  通过并行性增加容量
  通过复制容错
  将计算物理上靠近外部实体
  通过隔离实现安全性

但是：
  多个并发部分，复杂的交互
  必须处理部分故障
  实现性能潜力很棘手

为什么要学习这门课程？
  有趣——难题，强大的解决方案
  被真实系统使用——由大型网站的兴起推动
  活跃的研究领域——重要的未解决问题
  实践性——你将在实验中构建真实系统

课程结构

http://pdos.csail.mit.edu/6.824

课程团队：
  Frans Kaashoek，讲师
  Lily Tsai，助教
  Cel Skeggs，助教
  David Morejon，助教
  Jose Javier Gonzalez，助教

课程组成部分：
  讲座
  论文
  两次考试
  实验
  最终项目（可选）

讲座：
  重要思想、论文讨论和实验
  将会录像，在线提供

论文：
  研究论文，一些经典，一些新颖
  问题、思想、实现细节、评估
  许多讲座聚焦于论文
  请在课前阅读论文！
  每篇论文都有一个简短的问题需要你回答
  我们要求你发送一个关于论文的问题
  在讲座开始前提交问题&答案

考试：
  期中考试在课堂上进行
  期末考试在考试周期间
  主要关于论文和实验

实验：
  目标：深入理解一些重要技术
  目标：分布式编程经验
  第一个实验截止日期是下周五
  之后一段时间每周一个

实验1：MapReduce
实验2：使用Raft的容错复制
实验3：容错键值存储
实验4：分片键值存储

期末可选最终项目，2-3人一组。
  最终项目替代实验4。
  你想一个项目并与我们确认。
  代码、简短报告、最后一天的简短演示。

实验成绩取决于你通过的测试用例数量
  我们给你测试用例，所以你知道你是否会做得好

调试实验可能很耗时
  早点开始
  来参加助教答疑时间
  在Piazza上提问

主要主题

这是关于应用程序基础设施的课程。
  * 存储。
  * 通信。
  * 计算。

主要目标：隐藏分布式复杂性的抽象。
  在我们的探索中，有几个主题会反复出现。

主题：容错
  数千台服务器，大型网络 -> 总是有东西故障
    我们希望向应用程序隐藏这些故障。
  我们经常需要：
    可用性——即使出现故障，应用程序也能继续运行
    可恢复性——当故障被修复时，应用程序会恢复生命
  重要思想：复制服务器。
    如果一个服务器崩溃，可以使用其他服务器继续进行。
    很难做对
      服务器可能没有崩溃，只是暂时不可达
        但仍然在为客户端请求提供服务
    实验1、2和3

主题：一致性
  通用基础设施需要明确定义的行为。
    例如"Get(k)产生最近Put(k,v)的值。"
  实现良好行为很困难！
    "复制"服务器很难保持一致。

主题：性能
  目标：可扩展吞吐量
    N台服务器 -> 通过并行CPU、磁盘、网络获得N倍总吞吐量。
  随着N的增长，扩展变得更加困难：
    负载不平衡、掉队者、N中最慢的延迟。
    不可并行代码：初始化、交互。
    共享资源的瓶颈，例如网络。
  一些性能问题无法通过扩展轻松解决
    例如单个用户请求的快速响应时间
    例如所有用户都想更新相同数据
    通常需要更好的设计而不仅仅是更多计算机
  实验4

主题：容错、一致性和性能是敌人。
  强容错需要通信
    例如，发送数据到备份
  强一致性需要通信，
    例如Get()必须检查最近的Put()。
  许多设计只提供弱一致性，以获得速度。
    例如Get()不产生最新的Put()！
    对应用程序程序员来说是痛苦的，但可能是很好的权衡。
  在一致性/性能谱系中，许多设计点是可能的！

主题：实现
  RPC、线程、并发控制。
  实验...

历史背景
  局域网和互联网应用（自1980年代以来）
    10-100台机器：AFS
    互联网规模应用：DNS和电子邮件
  数据中心（1990年代末/2000年代初）
    拥有大量用户（数百万）和大量数据的网站
      Google、Yahoo、Facebook、Amazon、Microsoft等
      早期应用：网络搜索、电子邮件、购物等
    大量酷炫有趣系统的爆发
      > 1000台机器
      系统主要用于内部使用，工程师撰写关于它们的研究论文
  云计算
    用户将计算/存储外包给云提供商
    用户在云上运行自己的大型网站
    用户运行大量数据的大规模计算（例如，机器学习）
    => 大量新的面向用户的分布式系统基础设施
  当前状态：学术界和工业界非常活跃的研发领域
    很难跟上！
      6.824中的一些论文已经过时，但概念仍然相关
    6.824：侧重于容错/存储
      但也涉及通信和计算

案例研究：MapReduce

让我们讨论MapReduce（MR）作为案例研究
  很好地说明了6.824的主要主题
  影响巨大
  实验1的重点

MapReduce概述
  背景：在太字节级数据集上的多小时计算
    例如构建搜索索引、排序或分析网络结构
    只有数千台计算机才实用
    应用程序不是由分布式系统专家编写的
  总体目标：对非专业程序员来说简单
  程序员只需定义Map和Reduce函数
    通常是相当简单的顺序代码
  MR负责并隐藏所有分布式方面！

MapReduce作业的抽象视图
  输入（已经）被分割成M个文件
  Input1 -> Map -> a,1 b,1
  Input2 -> Map ->     b,1
  Input3 -> Map -> a,1     c,1
                    |   |   |
                    |   |   -> Reduce -> c,1
                    |   -----> Reduce -> b,2
                    ---------> Reduce -> a,2
  MR为每个输入文件调用Map()，产生k2,v2集合
    "中间"数据
    每个Map()调用是一个"任务"
  MR收集给定k2的所有中间v2，
    并将每个键+值传递给Reduce调用
  最终输出是来自Reduce()的<k2,v3>对集合

示例：词计数
  输入是数千个文本文件
  Map(k, v)
    将v分割成单词
    对于每个单词w
      emit(w, "1")
  Reduce(k, v)
    emit(len(v))

MapReduce扩展性好：
  N台"工作"计算机给你N倍吞吐量。
    Map()可以并行运行，因为它们不交互。
    Reduce()也是如此。
  所以你可以通过购买更多计算机来获得更多吞吐量。

MapReduce隐藏了许多细节：
  将应用程序代码发送到服务器
  跟踪哪些任务已完成
  将数据从Map移动到Reduce
  在服务器之间平衡负载
  从故障中恢复

然而，MapReduce限制了应用程序可以做什么：
  没有交互或状态（除了通过中间输出）。
  没有迭代，没有多阶段流水线。
  没有实时或流处理。

输入和输出存储在GFS集群文件系统上
  MR需要巨大的并行输入和输出吞吐量。
  GFS将文件分割到多个服务器上，以64MB块为单位
    Map并行读取
    Reduce并行写入
  GFS还在2或3个服务器上复制每个文件
  拥有GFS对MapReduce来说是一个巨大的胜利

什么可能会限制性能？
  我们关心因为这是要优化的事情。
  CPU？内存？磁盘？网络？
  在2004年，作者受限于网络容量。
    MR通过网络发送什么？
      Map从GFS读取输入。
      Reduce读取Map输出。
        可能和输入一样大，例如排序。
      Reduce将输出文件写入GFS。
    [图表：服务器、网络交换机树]
    在MR的全局洗牌中，一半流量通过根交换机。
    论文的根交换机：100到200千兆比特/秒，总计
      1800台机器，所以55兆比特/秒/机器。
      55很小，例如远小于磁盘或RAM速度。
  今天：相对于CPU/磁盘，网络和根交换机快得多。

一些细节（论文的图1）：
  一个协调器，向工作器分配任务并记录进度。
  1. 协调器将Map任务分配给工作器，直到所有Map完成
     Map将输出（中间数据）写入本地磁盘
     Map按哈希将输出分割，每个Reduce任务一个文件
  2. 所有Map完成后，协调器分配Reduce任务
     每个Reduce从（所有）Map工作器获取其中间输出
     每个Reduce任务在GFS上写入一个单独的输出文件

MR如何最小化网络使用？
  协调器尝试在存储其输入的GFS服务器上运行每个Map任务。
    所有计算机都运行GFS和MR工作器
    所以输入从本地磁盘（通过GFS）读取，而不是通过网络。
  中间数据只通过网络一次。
    Map工作器写入本地磁盘。
    Reduce工作器直接从Map工作器读取，而不是通过GFS。
  中间数据分割成包含许多键的文件。
    R远小于键的数量。
    大型网络传输更高效。

MR如何获得良好的负载平衡？
  如果N-1台服务器必须等待1台慢服务器完成，那是浪费和慢的。
  但有些任务可能比其他任务花费更长时间。
  解决方案：任务比工作器多得多。
    协调器将新任务分配给完成先前任务的工作器。
    所以没有任务大到主导完成时间（希望如此）。
    所以更快的服务器比较慢的服务器做更多任务，大约同时完成。

容错呢？
  即如果在MR作业期间工作器崩溃了怎么办？
  我们希望向应用程序程序员完全隐藏故障！
  MR必须从头重新运行整个作业吗？
    为什么不？
  MR只重新运行失败的Map()和Reduce()。
    假设MR运行一个Map两次，一个Reduce看到第一次运行的输出，
      另一个Reduce看到第二次运行的输出？
    正确性要求重新执行产生完全相同的输出。
    所以Map和Reduce必须是纯确定性函数：
      它们只允许查看它们的参数。
      没有状态，没有文件I/O，没有交互，没有外部通信。
  如果你想允许非函数式Map或Reduce？
    工作器故障将需要重新执行整个作业，
      或者你需要创建同步的全局检查点。

工作器崩溃恢复的细节：
  * Map工作器崩溃：
    协调器注意到工作器不再响应ping
    协调器知道它在该工作器上运行的Map任务
      那些任务的中间输出现在丢失，必须重新创建
      协调器告诉其他工作器运行那些任务
    如果Reduce已经获取了中间数据，可以省略重新运行
  * Reduce工作器崩溃。
    完成的任务正常——存储在GFS中，有副本。
    协调器在其他工作器上重新启动工作器未完成的任务。

其他故障/问题：
  * 如果协调器给两个工作器相同的Map()任务？
    也许协调器错误地认为一个工作器死了。
    它只会告诉Reduce工作器其中一个。
  * 如果协调器给两个工作器相同的Reduce()任务？
    它们都会尝试在GFS上写入相同的输出文件！
    原子GFS重命名防止混合；一个完整的文件将可见。
  * 如果单个工作器很慢——"掉队者"？
    也许由于硬件不稳定。
    协调器启动最后几个任务的第二个副本。
  * 如果工作器由于损坏的硬件或软件计算错误的输出？
    太糟了！MR假设"故障停止"CPU和软件。
  * 如果协调器崩溃？

当前状态？
  影响巨大（Hadoop、Spark等）。
  可能不再在Google使用。
    被Flume / FlumeJava取代（见Chambers等人的论文）。
    GFS被Colossus（没有好的描述）和BigTable取代。

结论
  MapReduce独自使大型集群计算流行起来。
  - 不是最高效或最灵活的。
  + 扩展性好。
  + 易于编程——故障和数据移动被隐藏。
  这些在实践中是很好的权衡。
  我们将在课程后面看到一些更高级的后续产品。
  实验愉快！