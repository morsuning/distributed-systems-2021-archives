6.824 2021 第3讲：GFS

Google文件系统
Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung
SOSP 2003

我们为什么要读这篇论文？
  分布式存储是一个关键抽象
    接口/语义应该是什么样的？
    它内部应该如何工作？
  GFS论文涉及6.824的许多主题
    并行性能、容错、复制、一致性
  好的系统论文——从应用程序到网络的细节
  成功的现实世界设计

为什么分布式存储很难？
  高性能 -> 在多个服务器上分片数据
  多个服务器 -> 持续故障
  容错 -> 复制
  复制 -> 潜在不一致
  更好的一致性 -> 低性能

我们希望什么一致性？
  理想模型：与单一服务器相同的行为
  服务器使用磁盘存储
  服务器一次执行一个客户端操作（即使是并发的）
  读取反映之前的写入
    即使服务器崩溃并重启
  因此：
    假设C1和C2并发写入，写入完成后，
      C3和C4读取。它们能看到什么？
    C1: Wx1
    C2: Wx2
    C3:     Rx?
    C4:         Rx?
    答案：要么是1要么是2，但两者必须看到相同的值。
  这是一个"强"一致性模型。
  但单一服务器容错性差。

为容错而进行的复制使强一致性变得复杂。
  一个简单但有缺陷的复制方案：
    两个副本服务器，S1和S2
    客户端并行向两者发送写入
    客户端向任一服务器发送读取
  在我们的例子中，C1和C2的写入消息可能在
    两个副本处以不同顺序到达
    如果C3读取S1，它可能看到x=1
    如果C4读取S2，它可能看到x=2
  或者如果S1接收到写入，但
    客户端在向S2发送写入之前崩溃了？
  那不是强一致性！
  更好的一致性通常需要通信来
    确保副本保持同步——可能很慢！
  性能和一致性之间有很多可能的权衡
    我们今天将看到一个

GFS

背景：
  许多谷歌服务需要一个大型快速的统一存储系统
    Mapreduce、爬虫/索引器、日志存储/分析、Youtube（？）
  全局（在单个数据中心内）：任何客户端可以读取任何文件
    允许应用程序间共享数据
  每个文件在多个服务器/磁盘上自动"分片"
    为了并行性能
    为了增加可用空间
  自动故障恢复
  每次部署只有一个数据中心
  只有谷歌应用程序/用户
  针对大文件的顺序访问；读取或追加
    即不是小项目的低延迟数据库

这在2003年有什么新意？他们如何让SOSP论文被接受？
  不是分布式、分片、容错的基本思想。
  巨大的规模。
  在工业中使用，现实世界的经验。
  弱一致性的成功使用。
  单一主服务器的成功使用。

整体结构
  客户端（库、RPC——但不是可见的UNIX FS）
  每个文件分割成独立的64 MB块
  块服务器，每个块复制3份
  每个文件的块分布在块服务器上
    为了并行读/写（例如MapReduce），并允许大文件
  单一主服务器（！）和主服务器副本
  工作分工：主服务器处理命名，块服务器处理数据

主服务器状态
  在RAM中（为了速度，必须相对较小）：
    文件名 -> 块句柄数组（非易失性）
    块句柄 -> 版本号（非易失性）
                    块服务器列表（易失性）
                    主服务器（易失性）
                    租约时间（易失性）
  在磁盘上：
    日志
    检查点

为什么用日志？和检查点？

为什么用大块？

当客户端C想要读取文件时的步骤是什么？
  1. C向主服务器M发送文件名和偏移量（如果未缓存）
  2. M找到该偏移量的块句柄
  3. M回复块服务器列表
     只有那些具有最新版本的
  4. C缓存句柄+块服务器列表
  5. C向最近的块服务器发送请求
     块句柄，偏移量
  6. 块服务器从磁盘上的块文件读取，返回

主服务器如何知道哪些块服务器有给定的块？

当C想要进行"记录追加"时的步骤是什么？
  论文的图2
  1. C询问M关于文件的最后一块
  2. 如果M看到块没有主服务器（或租约过期）：
     2a. 如果没有具有最新版本号的块服务器，错误
     2b. 从具有最新版本号的中选择主服务器P和副本
     2c. 增加版本号，写入磁盘上的日志
     2d. 告诉P和副本它们是谁，以及新版本号
     2e. 副本将新版本号写入磁盘
  3. M告诉C主服务器和副本
  4. C向所有服务器发送数据（只是临时的...），等待
  5. C告诉P追加
  6. P检查租约未过期，且块有空间
  7. P选择一个偏移量（在块的末尾）
  8. P写入块文件（一个Linux文件）
  9. P告诉每个副本偏移量，告诉追加到块文件
  10. P等待所有副本回复，或超时
      副本可以回复"错误"，例如磁盘空间不足
  11. P告诉C"好的"或"错误"
  12. 如果错误，C从头重试

GFS向客户提供什么一致性保证？
  需要是一种告诉应用程序如何使用GFS的形式。

这里有一个可能性：

  如果主服务器告诉客户端记录追加成功，那么
  任何随后打开文件并扫描它的读者将在某处看到
  追加的记录。

（但不是失败的追加不会可见，或者所有读者
 将看到相同的文件内容，或相同的记录顺序。）

我们如何思考GFS如何实现这个保证？
  查看它对各种故障的处理：
    崩溃、崩溃+重启、崩溃+替换、消息丢失、分区。
  询问GFS如何确保关键属性。

* 如果追加客户端在尴尬时刻失败了怎么办？
  有尴尬时刻吗？

* 如果追加客户端缓存了陈旧（错误）的主服务器怎么办？

* 如果读取客户端缓存了陈旧的副本列表怎么办？

* 主服务器崩溃+重启会导致它忘记文件吗？
  或者忘记哪些块服务器持有相关块？

* 两个客户端同时进行记录追加。
  它们会覆盖彼此的记录吗？

* 假设一个副本从未听到来自主服务器的追加命令。
  如果读取客户端从该副本读取怎么办？

* 如果主服务器在向所有副本发送追加之前崩溃了怎么办？
  没有看到追加的副本会被选为新的主服务器吗？

* 具有块的旧陈旧副本的块服务器S4离线。
  主服务器和所有活动副本崩溃。
  S4恢复生命（在主服务器和副本之前）。
  主服务器会选择S4（具有陈旧块）作为主服务器吗？
  最好有具有陈旧数据的主服务器，还是根本没有副本？

* 如果副本总是写入失败，主服务器应该做什么？
  例如，死机、磁盘空间不足或磁盘损坏。
  主服务器应该从副本集合中丢弃副本吗？
    然后向客户端追加返回成功？
  或者主服务器应该继续发送操作，让它们失败，
    从而使每个客户端写入请求都失败？

* 如果主服务器S1存活并服务客户端请求，
    但主服务器和S1之间的网络故障了怎么办？
  "网络分区"
  主服务器会选择新的主服务器吗？
  现在会有两个主服务器吗？
  以便追加去一个主服务器，读取去另一个？
    从而破坏一致性保证？
    "脑裂"

* 如果有分区的主服务器服务客户端追加，它的
  租约过期，主服务器选择新的主服务器，新的
  主服务器会有分区主服务器更新的最新数据吗？

* 如果主服务器完全失败了怎么办。
  替代品会知道死主服务器知道的一切吗？
  例如，每个块的版本号？主服务器？租约到期时间？

* 谁/什么决定主服务器死了，必须被替换？
  主服务器副本可以ping主服务器，如果没有响应就接管吗？

* 如果整个建筑发生电力故障怎么办？
  然后电力恢复，所有服务器重启。

* 假设主服务器想要创建新的块副本。
  也许是因为副本太少。
  假设它是文件中的最后一块，并且正在被追加。
  新副本如何确保它不会错过任何追加？
    毕竟它还不是副本之一。

* GFS是否会打破保证的任何情况？
  即追加成功，但后续读者看不到记录。
  所有主服务器副本永久丢失状态（永久磁盘故障）。
    可能更糟：结果将是"没有答案"，而不是"错误数据"。
    "故障停止"
  持有块的所有块服务器永久丢失磁盘内容。
    同样，故障停止；不是最坏的可能结果
  CPU、RAM、网络或磁盘产生错误值。
    校验和捕捉一些情况，但不是全部
  时间没有正确同步，所以租约不能正常工作。
    所以多个主服务器，也许写入去一个，读取去另一个。

GFS允许什么应用程序可见的异常行为？
  所有客户端会看到相同的文件内容吗？
    一个客户端可能看到另一个客户端根本看不到的记录吗？
    客户端读取文件两次会看到相同的内容吗？
  所有客户端会以相同顺序看到成功追加的记录吗？

这些异常会给应用程序带来麻烦吗？
  MapReduce怎么样？

没有异常需要什么——严格一致性？
  即所有客户端看到相同的文件内容。
  很难给出真正的答案，但这里有一些问题。
  * 主服务器应该检测重复的客户端写入请求。
    或者客户端不应该发出它们。
  * 所有副本应该完成每次写入，或者都不完成。
    也许临时写入直到所有都承诺完成它？
    在所有都同意执行之前不要暴露写入！
  * 如果主服务器崩溃，一些副本可能缺少最后几个操作。
    新主服务器必须与所有副本交谈以找到所有最近的操作，
    并与副本同步。
  * 为了避免客户端从陈旧的前副本读取，要么所有客户端
    读取必须去主服务器，要么副本也必须有租约。
  你将在实验2和3中看到所有这些！

性能（图3）
  读取的大聚合吞吐量（3个副本，条带化）
    16个块服务器总共94 MB/秒
      或每块服务器6 MB/秒
      那样好吗？
      一个磁盘顺序吞吐量约为30 MB/s
      一个网卡约为10 MB/s
    接近饱和网络（交换机间链路）
    所以：单个服务器性能低
        但可扩展性好
        哪个更重要？
    表3报告生产GFS为500 MB/秒，这很多
  对不同文件的写入低于可能的最大值
    作者归咎于他们的网络堆栈（但没有细节）
  对单个文件的并发追加
    受存储最后块的服务器限制
  15年后难以解释，例如磁盘有多快？

值得考虑的随机问题
  什么能很好地支持小文件？
  什么能支持数十亿文件？
  GFS能用作广域文件系统吗？
    副本在不同城市？
    所有副本在一个数据中心容错性不强！
  GFS需要多长时间从故障中恢复？
    主服务器/副本的？
    主服务器的？
  GFS如何处理慢速块服务器？

与GFS工程师的回顾访谈：
  http://queue.acm.org/detail.cfm?id=1594206
  文件数量是最大问题
    最终数字增长到表2的1000倍！
    难以放入主服务器RAM
    主服务器扫描所有文件/块进行GC很慢
  1000多个客户端对主服务器CPU负载过大
  应用程序必须设计来应对GFS语义
    和限制
  主服务器故障转移最初完全手动，几十分钟
  BigTable是许多小文件问题的一个答案
  Colossus显然将主服务器数据分片到多个主服务器

总结
  性能、容错、一致性的案例研究
    专门为MapReduce应用程序
  好的想法：
    全局集群文件系统作为通用基础设施
    命名（主服务器）与存储（块服务器）的分离
    并行吞吐量的分片
    大文件/块以减少开销
    主服务器排序写入
    租约以防止脑裂块服务器主服务器
  不那么好：
    单一主服务器性能
      耗尽RAM和CPU
    块服务器对小文件不是很有效
    缺乏自动故障转移到主服务器副本
    也许一致性太放松