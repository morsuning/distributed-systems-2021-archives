6.824 2021 第17讲：Facebook的Memcache扩展

Facebook的Memcache扩展，Nishtala等人，NSDI 2013

为什么要读这篇论文？
  它是一篇经验论文，不是关于新思想/技术
  有三种阅读方式：
    关于从一开始不认真对待一致性的警示故事
    主要使用现成软件实现超大容量的令人印象深刻的故事
    性能和一致性之间的根本斗争
  我们可以质疑他们的设计，但不能质疑他们的成功

网站如何应对用户增长？
  一个典型的随时间演变的故事：
  1. 单台机器运行web服务器+应用程序+数据库
     数据库提供持久化存储、崩溃恢复、事务、SQL
     应用程序查询数据库，格式化HTML等
     但是：随着负载增长，应用程序占用太多CPU时间
  2. 多个web前端，一个共享数据库
     一个简单的改变，因为web服务器+应用已经与存储分离
     前端是无状态的，所有共享（和并发控制）通过数据库
       无状态 -> 任何前端可以服务任何请求，前端崩溃没有损害
     但是：随着负载增长，需要更多前端，很快单台数据库服务器成为瓶颈
  3. 多个web前端，数据分片到数据库集群
     按键在数据库上分区数据
       应用查看键（例如用户），选择正确的数据库
     如果没有数据超级流行，数据库并行性良好
     痛苦 -- 跨分片事务和查询可能不工作
       很难分区得太细
     但是：数据库很慢，即使对于读取，为什么不缓存读取请求？
  4. 多个web前端，多个读取缓存，多个写入数据库
     成本效益高，因为读取密集且memcached比数据库快10倍
       memcached只是一个内存哈希表，非常简单
     复杂，因为数据库和memcached可能不同步
     脆弱，因为缓存未命中可能轻易使数据库过载
     （下一个瓶颈将是数据库写入——难以解决）

Facebook大型基础设施图景
  大量用户、好友列表、状态、帖子、点赞、照片
    新鲜/一致的数据不是关键
      人类是宽容的
  高负载：每秒十亿次操作
    这是一台数据库服务器吞吐量的10,000倍
      mysql约100,000个简单查询/秒
      约1,000个事务/秒
      memcached约1,000,000个get/put/秒
  多个数据中心（至少东西海岸）
  每个数据中心——"区域"：
    "真实"数据分片到MySQL数据库
    memcached层（mc）
    web服务器（memcached的客户端）
  每个数据中心的数据库包含完整副本
  西海岸是主区域，其他通过MySQL异步日志复制的辅助副本

FB应用程序如何使用mc？图1。
  FB使用mc作为"旁路"缓存
    真实数据在数据库中
    缓存值（如果有）应该与数据库相同
  读取：
    v = get(k) (计算hash(k)选择mc服务器)
    if v is nil {
      v = 从数据库获取
      set(k, v)
    }
  写入：
    v = 新值
    发送k,v到数据库
    delete(k)
  应用程序决定mc与数据库的关系
    mc对数据库一无所知

FB在mc中存储什么？
  论文没有说
  可能是userID -> name；userID -> 好友列表；postID -> text；URL -> 点赞
  从数据库查询派生的数据

论文教训：
  旁路缓存比看起来更棘手——一致性
    论文试图整合互不知情的存储层
  缓存是关键的：
    不是真的为了减少用户可见延迟
    主要是为了保护数据库免受巨大过载！
  人类用户可以容忍适度的读取陈旧性
  陈旧读取仍然可能是大问题
    想要避免无限陈旧性（例如完全错过delete()）
    想要读自己的写入
    更多缓存 -> 更多陈旧性来源
  巨大"扇出" => 并行获取，入站拥塞

让我们先谈谈性能
  论文大部分是关于避免陈旧数据
  但陈旧性只源于性能设计

性能来自由于多服务器的并行性
  许多活跃用户，许多web服务器（客户端）
  存储的两个基本并行策略：分区 vs 复制

分区还是复制会产生最多的mc吞吐量？
  分区：在mc服务器上划分键
  复制：在mc服务器上划分客户端
  分区：
    + 内存效率更高（每个k/v只有一个副本）
    + 如果没有键非常流行，效果很好
    - 每个web服务器必须与许多mc服务器通信（开销）
  复制：
    + 如果一些键非常流行，效果好
    + 较少的TCP连接
    - 可以缓存的总数据较少

性能和区域（第5节）

[图表：西部，数据库主分片，mc服务器，客户端 | 东部，数据库辅助分片，...
从数据库主到辅助的feed]

问：区域的重点是什么——多个完整副本？
   降低到用户的RTT（东海岸，西海岸）
   从本地mc和数据库快速本地读取
   （虽然写入昂贵：必须发送到主区域）
   可能为主站点故障提供热副本？

问：为什么不在区域上分区用户？
   即为什么东海岸用户的数据在东海岸区域，等等
   那么不需要复制：可能将硬件成本减半！
   但是：社交网络 -> 没有太多局部性
   对于例如电子邮件可能效果很好

问：尽管所有写入都强制发送到主区域，为什么性能还可以？
   写入比读取少得多
   发送写入到主区域可能需要100ms，对人类用户来说不算太糟
   用户不等待写入的所有效果完成
     即等待所有陈旧的缓存值被删除

区域内的性能（第4节）

[图表：数据库分片，多个集群，每个都有mc和客户端]

每个区域内*有*多个mc集群
  集群 == mc缓存服务器的完整集合
    即至少缓存数据的副本

为什么每个区域有多个集群？
  为什么不向单个集群添加越来越多的mc服务器？
  1. 向集群添加mc服务器对单个流行键没有帮助
     复制（每个集群一个副本）有帮助
  2. 集群中更多mc -> 每个客户端请求与更多服务器通信
     以及请求web服务器更多的入站拥塞
     客户端请求获取20到500个键！跨越许多mc服务器
     必须并行请求它们（否则总延迟太大）
     所以所有回复同时回来
     网络交换机，网卡缓冲区不足
  3. 为单个大集群构建网络困难
     统一的客户端/服务器访问
     所以横截面带宽必须很大——昂贵
     两个集群 -> 1/2的横截面带宽

但是——复制对不流行项目浪费RAM
  所有集群共享的"区域池"
  不流行的对象（不需要多个副本）
  应用软件决定什么放入区域池
  释放RAM以复制更流行的对象

启动新的mc集群是性能问题
  新集群有0%命中率
  如果客户端使用它，将产生数据库负载的大峰值
    如果通常1%未命中率，
      添加"冷"第二集群将导致50%操作未命中。
    即数据库负载50倍峰值！
  因此新集群的客户端首先从现有集群get()（4.3）
    并set()到新集群
    基本上是现有集群到新集群的懒惰复制
    现有集群2倍负载比数据库30倍负载更好

另一个过载问题：雷群效应
  一个客户端更新数据库并delete()一个键
  许多客户端get()但未命中
    它们都从数据库获取
    它们都set()
  不好：不必要的数据库负载
  mc只给第一个未命中的客户端一个"租约"
    租约 = 从数据库刷新的许可
    mc告诉其他人"几毫秒后重试get()"
  效果：只有一个客户端读取数据库并做set()
    其他人稍后重试get()并希望命中

如果mc服务器故障怎么办？
  不能让数据库服务器处理未命中——负载太大
  不能将负载转移到一个其他mc服务器——太多
  不能重新分区所有数据——耗时
  Gutter——空闲mc服务器池，客户端只在mc服务器故障后使用
  一段时间后，故障的mc服务器将被替换

问题：
  为什么客户端不向Gutter服务器发送无效化？
  我的猜测：会使delete()流量翻倍
    并向小的gutter池发送太多delete()
    因为任何键可能在gutter池中

重要的实际网络问题：
  n^2 TCP连接状态太多
    因此UDP用于客户端get()
  UDP不可靠或有序
    因此TCP用于客户端set()
    以及mcrouter来减少n^2中的n
  每包单个请求不高效（TCP或UDP）
    每包开销（中断等）太高
    因此mcrouter将许多请求批处理到每个包中

现在让我们谈谈一致性

他们的一致性目标是什么？
  写入直接到主数据库，带事务，所以写入是一致的
  读取呢？
  读取不总是看到最新写入
    例如因为不保证跨集群
  更像是"不超过几秒陈旧"
    即最终一致性
  *并且*写入者看到自己的写入（由于delete()）
    读自己的写入是一个大的驱动力

首先，数据库副本如何在区域间保持一致？
  一个区域是主区域
  主数据库向辅助区域的数据库分发更新日志
  辅助数据库应用
  辅助数据库是完整副本（不是缓存）
  数据库复制延迟可能相当大（许多秒）

他们如何保持mc内容与数据库内容一致？
  1. 数据库向可能缓存的所有mc服务器发送无效化（delete()）
     这是图6中的McSqueal
  2. 写入客户端也使本地集群中的mc无效
     为了读自己的写入

他们遇到了许多数据库vs mc一致性问题
  由于多个客户端从数据库读取并put()到mc时的竞争
  或者：没有一条有序的更新流路径

竞争和修复是什么？

竞争1：
  k不在缓存中
  C1 get(k)，未命中
  C1 v1 = 从数据库读取k
    C2在数据库中写入k = v2
    C2 delete(k)
  C1 set(k, v1)
  现在mc有陈旧数据，delete()已经发生
  将无限期保持陈旧，直到k下次被写入
  用租约解决——C1从mc获得租约，C2的delete()使租约无效，
    所以mc忽略C1的set
    键仍然缺失，所以下一个读取者将从数据库刷新它

竞争2：
  在冷集群预热期间
  记住：未命中时，客户端尝试在热集群get()，复制到冷集群
  k开始值为v1
  C1在数据库中将k更新为v2
  C1 delete(k) -- 在冷集群
  C2 get(k)，未命中 -- 在冷集群
  C2 v1 = 从热集群get(k)，命中
  C2 set(k, v1)到冷集群
  现在mc有陈旧v1，但delete()已经发生
    将无限期保持陈旧，直到键下次被写入
  用两秒延迟解决，只在冷集群使用
    C1 delete()后，冷mc忽略set()两秒
    到那时，delete()将（可能）通过数据库传播到热集群

竞争3：
  k开始值为v1
  C1在辅助区域
  C1在主数据库中将k更新为v2
  C1 delete(k) -- 本地区域
  C1 get(k)，未命中
  C1读取本地数据库 -- 看到v1，不是v2！
  稍后，v2从主数据库到达
  用"远程标记"解决
    C1 delete()标记键"远程"
    get()未命中产生"远程"
      告诉C1从*主*区域读取
    "远程"在从主区域到达新数据时清除

问：所有这些问题不都是因为客户端将数据库数据复制到mc引起的吗？
   为什么不让数据库向mc发送新值，使客户端只读mc？
     那么就没有竞争客户端更新等，只有有序写入
答：
  1. 数据库通常不知道如何为mc计算值
     通常客户端应用代码从数据库结果计算它们，
       即mc内容通常不简单地是字面数据库记录
  2. 会增加读自己写入的延迟
  3. 数据库不知道什么被缓存，最终会发送大量
     未缓存键的值

PNUTS确实采用了这种主更新所有副本的替代方法

FB/mc对存储系统设计者的教训？
  缓存对吞吐量生存至关重要，不仅仅是减少延迟
  需要灵活的工具来控制分区vs复制
  需要更好的思想来整合存储层与一致性

--- 参考资料

http://cs.cmu.edu/~beckmann/publications/papers/2020.osdi.cachelib.pdf