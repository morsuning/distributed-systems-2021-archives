6.824 2021 第6讲：Raft (1)

本次讲座
  今天：Raft选举和日志处理（实验2A, 2B）
  下次：Raft持久化、客户端行为、快照（实验2C, 2D）

在我们见过的容错系统中的一个模式
  * MR复制计算但依赖于单一主服务器来组织
  * GFS复制数据但依赖于主服务器来选择主服务器
  * VMware FT复制服务但依赖于测试和设置来选择主服务器
  都依赖于单一实体来做关键决策
    好处：单一实体的决策避免脑裂

脑裂是如何产生的，为什么它是有害的？
  假设我们在复制一个测试和设置服务
    客户端请求将状态设置为1，服务器用先前状态回复
    只有一个客户端应该得到带有"0"的回复！！！
    它是一个锁，只有一个请求者应该得到它
  [C1, C2, S1, S2]
  假设客户端C1可以联系副本S1，但不能联系副本S2
  C1应该只与副本S1一起进行吗？
  如果S2真的崩溃了，C1必须在没有S2的情况下进行，
    否则服务不容错！
  如果S2正常运行但网络阻止C1联系S2，
    C1不应该在没有S2的情况下进行，
    因为S2可能存活并为客户端C2服务
  有了这个设置，我们面临一个恶劣的选择：
    要么尽管复制却没有容错能力，要么
    可能因脑裂而导致操作错误

问题：计算机无法区分"服务器崩溃"与"网络中断"
  症状相同：对网络查询没有响应
  坏情况通常被称为"网络分区"：
    C1可以与S1交谈，C2可以与S2交谈，
    但C1+S1看不到来自C2+S2的响应
  这个困难长期以来似乎不可克服
  似乎需要外部代理（人类）来决定何时切换
    或一个完全可靠的服务器（FT的测试和设置服务器）
    或一个完全可靠的网络（所以"无响应"=="崩溃"）
  但这些都是单点故障——不理想
  能做得更好吗？

处理分区的大洞见：多数投票
  需要奇数个服务器，例如3个
  需要多数同意才能做任何事情——3个中的2个
  为什么多数有助于避免脑裂？
    最多一个分区可以有多数
    打破了我们只用两台服务器时看到的对称性
  注意：多数是所有服务器中的多数，不仅仅是存活服务器中的多数
  更一般地，2f+1可以容忍f个故障服务器
    因为剩余的f+1是2f+1的多数
    如果超过f个故障（或无法联系），没有进展
  通常被称为"法定人数"系统

多数的一个关键属性是任何两个必须相交
  例如，Raft领导人选举的连续多数必须重叠
    交集可以传达关于先前决策的信息

两个分区容忍复制方案在1990年左右发明，
  Paxos和视图标记复制
  在过去15年中，这项技术看到了很多现实世界的使用
  Raft论文是现代技术的一个很好的介绍

*** 主题：Raft概述

使用Raft的状态机复制——以实验3为例：
  [图：客户端、3个副本、k/v层+状态、raft层+日志]
  Raft是包含在每个副本中的库

一个客户端命令的时间图
  [C, L, F1, F2]
  客户端向领导人中的k/v层发送Put/Get"命令"
  领导人将命令添加到日志
  领导人向追随者发送AppendEntries RPC
  追随者将命令添加到日志
  领导人等待来自简单多数（包括自己）的回复
  如果多数将其放入日志，条目就"提交"了
    提交意味着即使故障也不会被遗忘
    多数 -> 将被下一个领导人的投票请求看到
  领导人执行命令，回复客户端
  领导人在下一个AppendEntries中"捎带"提交信息
  一旦领导人说已提交，追随者就执行条目

为什么要有日志？
  服务保持状态机状态，例如键/值数据库
    为什么那不够？
  日志对命令排序
    帮助副本就单一执行顺序达成一致
    帮助领导人确保追随者有相同的日志
  日志存储暂定命令直到提交
  日志存储命令以防领导人必须重新发送给追随者
  日志持久存储命令以在重启后重放

服务器的日志是否彼此完全相同？
  不：一些副本可能滞后
  不：我们将看到它们可以临时有不同的条目
  好消息：
    它们最终会收敛到相同
    提交机制确保服务器只执行稳定条目

实验2 Raft接口
  rf.Start(command) (index, term, isleader)
    实验3 k/v服务器的Put()/Get() RPC处理程序调用Start()
    Start()只在领导人上有意义
    开始对新日志条目的Raft同意
      添加到领导人的日志
      领导人发出AppendEntries RPC
      Start()返回而不等待RPC回复
      k/v层的Put()/Get()必须等待提交，在applyCh上
    如果服务器在提交前失去领导权，同意可能失败
      那么命令可能丢失，客户端必须重新发送
    isleader：如果此服务器不是领导人，则为false，客户端应该尝试另一个
    term：currentTerm，帮助调用者检测领导人是否后来被降级
    index：要监视的日志条目以查看命令是否已提交
  ApplyMsg，带有Index和Command
    每个对等方为每个提交的条目在applyCh上发送ApplyMsg
    每个对等方的本地服务代码执行，更新本地副本状态
    领导人向等待的客户端RPC发送回复

Raft设计有两个主要部分：
  选举新领导人
  尽管故障确保相同的日志

*** 主题：领导人选举（实验2A）

为什么需要领导人？
  确保所有副本以相同顺序执行相同命令
  （一些设计，例如Paxos，没有领导人）

Raft为领导人序列编号
  新领导人 -> 新任期
  一个任期最多有一个领导人；可能没有领导人
  编号帮助服务器跟随最新的领导人，而不是被取代的领导人

Raft对等方何时开始领导人选举？
  当它在"选举超时"时间内没有听到当前领导人的消息时
  增加本地currentTerm，尝试收集选票
  注意：这可能导致不必要的选举；那很慢但安全
  注意：旧领导人可能仍然存活并认为它是领导人

如何确保一个任期最多有一个领导人？
  （图2 RequestVote RPC和服务器规则）
  领导人必须得到多数服务器的"赞成"票
  每个服务器每个任期只能投一票
    如果是候选人，投票给自己
    如果不是候选人，投票给第一个询问的（在图2规则内）
  最多一个服务器可以得到给定任期的多数票
    -> 最多一个领导人，即使网络分区
    -> 即使一些服务器故障，选举也能成功

服务器如何了解新选举的领导人？
  新领导人看到来自多数的赞成票
  其他人看到带有更高任期号的AppendEntries心跳
    即来自新领导人
  心跳抑制任何新选举

选举可能因两个原因失败：
  * 少于多数的服务器可达
  * 同时候选人分割选票，没有一个得到多数

如果选举不成功会发生什么？
  另一个超时（没有心跳），新选举（和新任期）
  更高任期优先，旧任期的候选人退出

Raft如何避免分割选票？
  每个服务器选择随机选举超时
  [服务器超时到期时间图]
  随机性打破服务器间的对称性
    一个将选择最低随机延迟
  希望有足够时间在下一个超时到期前选举
  其他人将看到新领导人的AppendEntries心跳并且
    不成为候选人
  随机延迟是网络协议中的常见模式

如何选择选举超时？
  * 至少几个心跳间隔（以防网络丢弃心跳）
    避免不必要的选举，浪费时间
  * 随机部分足够长以让一个候选人在下一个开始前成功
  * 足够短以快速反应故障，避免长暂停
  * 足够短以允许测试者生气前进行几次重试
    测试者要求选举在5秒或更短时间内完成

如果旧领导人不知道新领导人被选举了怎么办？
  也许旧领导人没有看到选举消息
  也许旧领导人在少数网络分区中
  新领导人意味着多数服务器已经增加了currentTerm
    所以旧领导人（用旧任期）不能得到AppendEntries的多数
    所以旧领导人不会提交或执行任何新日志条目
    因此没有脑裂
    但少数可能接受旧服务器的AppendEntries
      所以日志可能在旧任期末尾分歧

---

Raft vs. Paxos: https://dl.acm.org/doi/10.1145/3293611.3331595